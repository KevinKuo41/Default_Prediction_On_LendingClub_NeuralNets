# Rebalancing Data
#### In the dataset, 29.7% of the samples are defaulters and 70.3% of those are non-defaulters so we know this is a imbalanced dataset. To improve the performance of the Machine Learning Technique, we implement rebalancing approach to adjust the data. 3 rebalancing approaches - Oversampling, Undersampling, SMOTE are the most often used methods. <br><br> Since the 42 feature variables include some catergorical features, the SMOTE approach is not a suitable rebalancing tool for our dataset. (The main reason is that the implementation of SMOTE is based on the simulation with applying KNN. However, in catergorical features, the different figures only mean different classes instead of the real distance between samples.) <br><br> Thus, we test the implementation of the Oversampling and Undersampling approach on both Random Forest and XGBoost models, and choose the one with better Average Precision and F1-Score to conduct further model training. <br> (The scores below are the average scores of 5-fold split implementation on the training dataset of 75% samples)
